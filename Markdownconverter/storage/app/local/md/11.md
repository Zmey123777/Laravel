## Install OpenSearch (ElasticSearch free implementation)

&copy; 2022 Innovative Scaling Technologies Inc.


Most of shell commands are executed on microk8s node.


### Activate required MicroK8s extensions


* Make sure that the following extensions are also active:

    * `dns`
    * `ha-cluster`
    * `helm3`
    * `dashboard`
    * `metrics-server`
    * `ingress`
    * `metallb`
    * `hostpath-storage`

  `hostpath-storage` extension can be installed for local host persistent volumes.
  It's deprecated on multi-node MicroK8s cluster.

  Use `microk8s status` shell command to list enabled extensions.

* Check `vm.max_map_count` kernel runtime parameter on all K8s cluster nodes:

  ```shell
  sysctl -n vm.max_map_count
  ```

  If this value on a node is less than `262144`, increase it at least to this value:

  ```shell
  sudo sysctl -w vm.max_map_count=262144
  echo 'vm.max_map_count = 262144' | sudo tee -a /etc/sysctl.conf
  ```


### Make sure that correct default storageClass is installed

```shell
 kubectl get sc
NAME                   PROVISIONER                                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
microk8s-hostpath      microk8s.io/hostpath                            Delete          WaitForFirstConsumer   false                  13d
nfs-client (default)   cluster.local/nfs-subdir-external-provisioner   Delete          Immediate              true                   7d
```

NFS based storageClass `nfs-client` is default here.

### Add OpenSearch Helm repository

```shell
helm repo add opensearch https://opensearch-project.github.io/helm-charts/
```

#### Check if OpenSearch Helm repository is added:
```shell
# helm repo list
NAME         URL                                                              
opensearch   https://opensearch-project.github.io/helm-charts/                
```

#### List Helm charts (packages) in OpenSearch repository:
```shell
# helm repo update
# helm search repo opensearch
NAME                            	CHART VERSION	APP VERSION	DESCRIPTION                           
opensearch/opensearch           	2.0.1        	2.0.0      	A Helm chart for OpenSearch           
opensearch/opensearch-dashboards	2.0.1        	2.0.0      	A Helm chart for OpenSearch Dashboards
```

### Download and install OpenSearch Helm chart

* Download OpenSearch Helm package and extract it to `opensearch` directory:

```shell
helm pull opensearch/opensearch --untar
```


* Create custom values file `values-opensearch.yaml`:

  ```shell
  cat <<'EOF' >values-opensearch.yaml
  replicas: 3
  persistence:
    size: 2Gi
    storageClass: nfs-client
  opensearchJavaOpts: "-Xms1024m -Xmx1024m"
  #extraEnvs:
  #- name: DISABLE_INSTALL_DEMO_CONFIG
  #  value: "true"
  #service:
  #  type: LoadBalancer
  #  externalTrafficPolicy: Local
  EOF
  ```

  Make sure that `persistence.storageClass` value exists and well configured,
  otherwise default storageClass will be used:
  ```shell
  kubectl get sc
  ```

  Increase `persistence.size` and `opensearchJavaOpts` in production OpenSearch installation
  to match actual requirements.
  The value of `persistence.size` should be
  [enough](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/sizing-domains.html)
  to store OpenSearch indices.
  RAM size in `opensearchJavaOpts` option should be enough for acceptable OpenSearch
  performance.

  Use `nginx.service.type: LoadBalancer` to change default Nginx service type and assign
  "external" IP address. `metallb` extension should be enabled in this case.


* Debug custom `values.yaml` and check K8s manifests generated by OpenSearch Helm chart

  Custom `values.yaml` is usually created step-by-step. K8s manifests can be
  previewed each time with the following command to debug the changes:

  ```shell
  helm install --debug --dry-run --disable-openapi-validation openseach ./opensearch --values values-opensearch.yaml | less
  ```

  See `values.yaml` in chart directory (`./opensearch`) for more options.


* Install OpenSearch Helm chart:

  ```shell
  helm install opensearch ./opensearch --values values-opensearch.yaml
  ```


### Monitor pod status

```shell
kubectl get pods -l app.kubernetes.io/component=opensearch-cluster-master -w
------------------------
NAME                          READY   STATUS    RESTARTS       AGE
opensearch-cluster-master-0   1/1     Running   14 (45h ago)   55d
^C
------------------------
````

Wait for `Running` status. If an error status is indicated on a pod, view pod events:

```shell
kubectl describe pod opensearch-cluster-master-0
```

and pod logs:

```shell
kubectl logs -f opensearch-cluster-master-0
...
^C
```

If error message is appear, resolve the issue, for example:

```shell
kubectl logs opensearch-cluster-master-0
------------------------
[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
ERROR: OpenSearch did not exit normally - check the logs at /usr/share/opensearch/logs/opensearch-cluster.log
------------------------
```

It's obviously in this case that `vm.max_map_count` should be increased on all K8s
cluster nodes:

```shell
sudo sysctl -w vm.max_map_count=262144
echo 'vm.max_map_count = 262144' | sudo tee -a /etc/sysctl.conf
```

Then re-install Helm chart.

```shell
helm uninstall opensearch
helm install opensearch ./opensearch --values values-opensearch.yaml
```


### Check if OpenSearch service is functioning

* Request OpenSearch service internally from inside of container:

```shell
# kubectl exec -it opensearch-cluster-master-0 -- curl -sk https://localhost:9200 -u admin:admin
{
  "name" : "opensearch-cluster-master-0",
  "cluster_name" : "opensearch-cluster",
  "cluster_uuid" : "WT0QiDsJS0GdOmqEQoRCtw",
  "version" : {
    "distribution" : "opensearch",
    "number" : "2.0.1",
    "build_type" : "tar",
    "build_hash" : "6462a546240f6d7a158519499729bce12dc1058b",
    "build_date" : "2022-06-15T08:47:42.243126494Z",
    "build_snapshot" : false,
    "lucene_version" : "9.1.0",
    "minimum_wire_compatibility_version" : "7.10.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "The OpenSearch Project: https://opensearch.org/"
}
```

NB: username and password are set to `admin` in demo Opensearch configuration


* Request OpenSearch with service IP address:

```shell
# kubectl get svc
NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)             AGE
opensearch-cluster-master ClusterIP  10.152.183.113  <none>       9200/TCP,9300/TCP   172m
# curl -sk https://10.152.183.113:9200 -u admin:admin
{
  "name" : "opensearch-cluster-master-0",
  ...
}
```


## OpenSearch: security plugin

OpeSearch is installed by default in demo mode: demo SSL certificates,
`admin` user has `admin` password etc.

Secure production installation includes:

* Custom SSL certificates
* Secure passwords
* Backend configuration

### Install custom SSL certificates

[Sample script](https://opensearch.org/docs/latest/security-plugin/configuration/generate-certificates/#sample-script)
can be used to develop a script to generate custom certificates for
admin, nodes and client.

* Create `files/certificates` directory in OpenSearch Helm template:

  ```shell
  mkdir -p opensearch/files/certificates
  cd opensearch/files/certificates
  ```


* Create a shell script to generate SSL certificates:

  ```shell
  cat <<'EOF' >gen-crt
  #!/bin/bash
  #
  # Generate certificates for OpenSearch security plugin
  #
  # Create self-signed root CA before other certificates
  #   ./gen-crt
  #
  # Pass base name of root CA files (CA key: CA_BASE_NAME-key.pem, CA cert: BASE_NAME.pem):
  #   ./gen-crt CA_BASE_NAME
  set -e
  DEFAULT_CA_BASE_NAME=root-ca
  
  SUBJ='/C=US/ST=DC/L=Washington/O=InnoScale'
  DAYS=7300
  NNOD=3
  DOM_PTN='opensearch-cluster-master-%s'
  
  # Root CA base name
  CA_BASE_NAME=${1:-$DEFAULT_CA_BASE_NAME}
  CA_KEY="$CA_BASE_NAME"-key.pem
  CA="$CA_BASE_NAME".pem
  
  # Generate root CA if root CA base name is not passed
  if [[ -z "$1" ]]; then
    # Root CA
    openssl genrsa -out root-ca-key.pem 2048
    openssl req -new -x509 -sha256 -key "$CA_KEY" -subj "$SUBJ/CN=ROOT" -out "$CA" -days "$DAYS"
  else
    [[ -f "$CA_KEY" ]] || { echo "** File not found: '$CA_KEY'"; exit 1; }
    [[ -f "$CA"     ]] || { echo "** File not found: '$CA'"    ; exit 1; }
  fi
  
  # Create a certificate
  # $1 -- file prefix
  # $2 -- subj CN suffix
  function make_cert() {
    local FILE="$1"
    local CN="$2"
    echo "Generating a certificate '$CN'..."
    openssl genrsa -out "$FILE"-key-temp.pem 2048
    openssl pkcs8 -inform PEM -outform PEM -in "$FILE"-key-temp.pem -topk8 -nocrypt -v1 PBE-SHA1-3DES -out "$FILE"-key.pem
    openssl req -new -key "$FILE"-key.pem -subj "$SUBJ/CN=$CN" -out "$FILE".csr
    openssl x509 -req -in "$FILE".csr -CA "$CA" -CAkey "$CA_KEY" -CAcreateserial -sha256 -out "$FILE".pem -days "$DAYS"
    rm -f "$FILE"-key-temp.pem
    rm -f "$FILE".csr
  }
  
  # Admin cert
  make_cert admin ADMIN
  # Node cert
  make_cert node node.example.com
  # Client cert
  make_cert client CLIENT
  echo Ok.
  EOF
  ```

  and run it:

  ```shell
  chmod +x gen-crt
  ./gen-crt
  cd ../../..
  ```

  The following files have been created:

    * admin-key.pem
    * admin.pem
    * client-key.pem
    * client.pem
    * node-key.pem
    * node.pem
    * root-ca-key.pem
    * root-ca.pem

  If you already have CA key and CA certificate, copy them to
  `root-ca-key.pem` and `root-ca.pem` respectively, then run

  ```shell
  ./gen-crt root-ca
  ```


* Create a named template in Helm to pack the files in specified directory
  to K8s secrets:

  Add to `opensearch/templates/_helpers.tpl` the `secFiles` named template definition:

  ```shell
  cat <<'EOF' >>opensearch/templates/_helpers.tpl
  
  {{/*
  Include files from given chart directory to a secret. The files are also interpreted as Helm templates.
  Usage example:
  {{ include "secFiles" (list $ "files/certificates/") }}
  Insert it after "data" key to generate secret data.
  */}}
  {{- define "secFiles" -}}
  {{- $top := index . 0 }}
  {{- $dir := index . 1 }}
  {{- range $path, $_ := $top.Files.Glob (printf "%s*" $dir) }}
    {{ $path | trimPrefix $dir }}: {{ tpl ($top.Files.Get $path) $top | b64enc }}
  {{- end }}
  {{- end }}
  EOF
  ```


* Create K8s secrets from certificate files as opensearch/templates/secret.yaml:

  ```shell
  cat <<'EOF' >opensearch/templates/secret.yaml
  apiVersion: v1
  kind: Secret
  metadata:
    name: {{ template "opensearch.uname" . }}-secret
    labels:
      {{- include "opensearch.labels" . | nindent 4 }}
  data:
  {{- include "secFiles" (list $ "files/certificates/") }}
  EOF
  ```

  The file `opensearch/templates/configmap.yaml` was used as a prototype.


* Configure mounting the secrets to container directory
  `/usr/share/opensearch/config/certs/`.

  Add the lines to custom values file:

  ```shell
  cat <<'EOF' >>values-opensearch.yaml

  extraVolumes:
    - name: opensearch-certs
      secret:
        secretName: opensearch-cluster-master-secret

  extraVolumeMounts:
    - name: opensearch-certs
      mountPath: "/usr/share/opensearch/config/certs/"
      readOnly: true
  EOF
  ```

  **NB**: Replace `opensearch` prefix in `secretName` value with
  actual Helm application name
  if a different application name was used in `helm install` command.


* Add `opensearch.yaml` definition in custom `values.yaml`:

  ```shell
  cat <<'EOF' >>values-opensearch.yaml
  
  # such as opensearch.yml and log4j2.properties
  config:
    opensearch.yml: |
      cluster.name: opensearch-cluster
  
      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0
  
      # # set to 1 to allow single node clusters
      # discovery.zen.minimum_master_nodes: 1
  
      # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
      # discovery.type: single-node
  
      plugins:
        security:
          ssl:
            transport:
              pemcert_filepath: certs/node.pem
              pemkey_filepath: certs/node-key.pem
              pemtrustedcas_filepath: certs/root-ca.pem
              enforce_hostname_verification: false
            http:
              enabled: true
              pemcert_filepath: certs/node.pem
              pemkey_filepath: certs/node-key.pem
              pemtrustedcas_filepath: certs/root-ca.pem
          allow_unsafe_democertificates: true
          allow_default_init_securityindex: true
          authcz:
            admin_dn:
              - CN=ADMIN,O=InnoScale,L=Washington,ST=DC,C=US
          nodes_dn:
            - CN=node.example.com,O=InnoScale,L=Washington,ST=DC,C=US
          audit.type: internal_opensearch
          enable_snapshot_restore_privilege: true
          check_snapshot_restore_write_privileges: true
          restapi:
            roles_enabled: ["all_access", "security_rest_api_access"]
          system_indices:
            enabled: true
            indices:
              [
                ".opendistro-alerting-config",
                ".opendistro-alerting-alert*",
                ".opendistro-anomaly-results*",
                ".opendistro-anomaly-detector*",
                ".opendistro-anomaly-checkpoints",
                ".opendistro-anomaly-detection-state",
                ".opendistro-reports-*",
                ".opendistro-notifications-*",
                ".opendistro-notebooks",
                ".opendistro-asynchronous-search-response*",
            ]
  securityConfig:
    config:
      path: /usr/share/opensearch/config/opensearch-security
      dataComplete: false
  EOF
  ```

  Specify security configuration directory in `securityConfig.path`.
  `securityConfig.dataComplete` is set to `false` to provide mounting
  security configuration filed individually, not as the whole directory.


### Install custom user passwords

* Add the lines to custom values file under `securityConfig.config`:

  ```shell
  cat <<'EOF' >>values-opensearch.yaml
      data:
        internal_users.yml: |-
          ---
          # This is the internal user database
          # The hash value is a bcrypt hash and can be generated with plugin/opensearch-security/tools/hash.sh
          _meta:
            type: "internalusers"
            config_version: 2
          
          # Define your internal users here
          ## Demo users
          admin:
            hash: "$2y$12$9xjL8a6IwdWKCjIzfUfk.ea./KX5W5ZL50ob9R.c732wm51x4noB."
            reserved: true
            backend_roles:
            - "admin"
            description: "Demo admin user"
          
          kibanaserver:
            hash: "$2y$12$jg1SHTMyKmu9MoQeHPrMU.tIHWnirF3U.2eP3SgWUkMTUjQNAYVXe"
            reserved: true
            description: "Demo OpenSearch Dashboards user"
          
          kibanaro:
            hash: "$2y$12$FhrbxRtEIDBSu.l9oXno6.z7Azie/9bIIWutVWyX0WVyclGiFwPHi"
            reserved: false
            backend_roles:
            - "kibanauser"
            - "readall"
            attributes:
              attribute1: "value1"
              attribute2: "value2"
              attribute3: "value3"
            description: "Demo OpenSearch Dashboards read only user"
          
          logstash:
            hash: "$2y$12$dcnuacyHIUeHq/zCMYZc..vwzvFzMb8d.b0ZzZpPcf3voOQ0Kjxoq"
            reserved: false
            backend_roles:
            - "logstash"
            description: "Demo logstash user"
          
          readall:
            hash: "$2y$12$wMhQaO8hXJiMo.mk.ZpnpeUb9W3srGmzfpTUgyxsangA5k0ChXHT6"
            reserved: false
            backend_roles:
            - "readall"
            description: "Demo readall user"
          
          snapshotrestore:
            hash: "$2y$12$Wt18F7AA37xRqA9twqTWJ.rFYnoqocH7Bv59SUAd02luGhP249kqO"
            reserved: false
            backend_roles:
            - "snapshotrestore"
            description: "Demo snapshotrestore user"
  EOF
  ```

  `hash` values can be generated with `hash.sh` script in OpenSearch container.
  For example let's get a hash for the password "Shai5hoo@cahpoh":

  ```shell
  kubectl exec opensearch-cluster-master-0 -- /usr/share/opensearch/plugins/opensearch-security/tools/hash.sh -p 'Shai5hoo@cahpoh'
  $2y$12$9xjL8a6IwdWKCjIzfUfk.ea./KX5W5ZL50ob9R.c732wm51x4noB.
  ```

  `hash` values generated for the above configuration:

  ```
  Shai5hoo@cahpoh $2y$12$9xjL8a6IwdWKCjIzfUfk.ea./KX5W5ZL50ob9R.c732wm51x4noB. admin
  ooGohg=oosa5ath $2y$12$jg1SHTMyKmu9MoQeHPrMU.tIHWnirF3U.2eP3SgWUkMTUjQNAYVXe kibanaserver
  ieRoo7cie.ree3t $2y$12$FhrbxRtEIDBSu.l9oXno6.z7Azie/9bIIWutVWyX0WVyclGiFwPHi kibanaro
  jee8eig+aiW9toh $2y$12$dcnuacyHIUeHq/zCMYZc..vwzvFzMb8d.b0ZzZpPcf3voOQ0Kjxoq logstash
  ing%ahLohph7ees $2y$12$wMhQaO8hXJiMo.mk.ZpnpeUb9W3srGmzfpTUgyxsangA5k0ChXHT6 readall
  oow6ohCh.u1eeco $2y$12$Wt18F7AA37xRqA9twqTWJ.rFYnoqocH7Bv59SUAd02luGhP249kqO snapshotrestore
  ```


### OpenSearch backend configuration

* Add the lines to custom values file under `securityConfig.config.data`:

  ```shell
  cat <<'EOF' >>values-opensearch.yaml
        config.yml: |-
          ---
          _meta:
            type: "config"
            config_version: 2
          
          config:
            dynamic:
              # Set filtered_alias_mode to 'disallow' to forbid more than 2 filtered aliases per index
              # Set filtered_alias_mode to 'warn' to allow more than 2 filtered aliases per index but warns about it (default)
              # Set filtered_alias_mode to 'nowarn' to allow more than 2 filtered aliases per index silently
              #filtered_alias_mode: warn
              #do_not_fail_on_forbidden: false
              #kibana:
              # Kibana multitenancy
              #multitenancy_enabled: true
              #server_username: kibanaserver
              #index: '.kibana'
              http:
                anonymous_auth_enabled: false
                xff:
                  enabled: false
                  internalProxies: '192\.168\.0\.10|192\.168\.0\.11' # regex pattern
                  #internalProxies: '.*' # trust all internal proxies, regex pattern
                  #remoteIpHeader:  'x-forwarded-for'
                  ###### see https://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html for regex help
                  ###### more information about XFF https://en.wikipedia.org/wiki/X-Forwarded-For
                  ###### and here https://tools.ietf.org/html/rfc7239
                  ###### and https://tomcat.apache.org/tomcat-8.0-doc/config/valve.html#Remote_IP_Valve
              authc:
                kerberos_auth_domain:
                  http_enabled: false
                  transport_enabled: false
                  order: 6
                  http_authenticator:
                    type: kerberos
                    challenge: true
                    config:
                      # If true a lot of kerberos/security related debugging output will be logged to standard out
                      krb_debug: false
                      # If true then the realm will be stripped from the user name
                      strip_realm_from_principal: true
                  authentication_backend:
                    type: noop
                basic_internal_auth_domain:
                  description: "Authenticate via HTTP Basic against internal users database"
                  http_enabled: true
                  transport_enabled: true
                  order: 4
                  http_authenticator:
                    type: basic
                    challenge: true
                  authentication_backend:
                    type: intern
                proxy_auth_domain:
                  description: "Authenticate via proxy"
                  http_enabled: false
                  transport_enabled: false
                  order: 3
                  http_authenticator:
                    type: proxy
                    challenge: false
                    config:
                      user_header: "x-proxy-user"
                      roles_header: "x-proxy-roles"
                  authentication_backend:
                    type: noop
                jwt_auth_domain:
                  description: "Authenticate via Json Web Token"
                  enabled: true  
                  #http_enabled: false
                  #transport_enabled: false
                  order: 0
                  http_authenticator:
                    type: jwt
                    challenge: false
                    config:
                      signing_key: "aWZpa3NyCg=="
                      jwt_header: "Authorization"
                      jwt_url_parameter: null
                      roles_key: null
                      subject_key: null
                  authentication_backend:
                    type: noop
                clientcert_auth_domain:
                  description: "Authenticate via SSL client certificates"
                  http_enabled: false
                  transport_enabled: false
                  order: 2
                  http_authenticator:
                    type: clientcert
                    config:
                      username_attribute: cn #optional, if omitted DN becomes username
                    challenge: false
                  authentication_backend:
                    type: noop
                ldap:
                  description: "Authenticate via LDAP or Active Directory"
                  http_enabled: false
                  transport_enabled: false
                  order: 5
                  http_authenticator:
                    type: basic
                    challenge: false
                  authentication_backend:
                    # LDAP authentication backend (authenticate users against a LDAP or Active Directory)
                    type: ldap
                    config:
                      # enable ldaps
                      enable_ssl: false
                      # enable start tls, enable_ssl should be false
                      enable_start_tls: false
                      # send client certificate
                      enable_ssl_client_auth: false
                      # verify ldap hostname
                      verify_hostnames: true
                      hosts:
                      - localhost:8389
                      bind_dn: null
                      password: null
                      userbase: 'ou=people,dc=example,dc=com'
                      # Filter to search for users (currently in the whole subtree beneath userbase)
                      # {0} is substituted with the username
                      usersearch: '(sAMAccountName={0})'
                      # Use this attribute from the user as username (if not set then DN is used)
                      username_attribute: null
              authz:
                roles_from_myldap:
                  description: "Authorize via LDAP or Active Directory"
                  http_enabled: false
                  transport_enabled: false
                  authorization_backend:
                    # LDAP authorization backend (gather roles from a LDAP or Active Directory, you have to configure the above LDAP authentication backend settings too)
                    type: ldap
                    config:
                      # enable ldaps
                      enable_ssl: false
                      # enable start tls, enable_ssl should be false
                      enable_start_tls: false
                      # send client certificate
                      enable_ssl_client_auth: false
                      # verify ldap hostname
                      verify_hostnames: true
                      hosts:
                      - localhost:8389
                      bind_dn: null
                      password: null
                      rolebase: 'ou=groups,dc=example,dc=com'
                      # Filter to search for roles (currently in the whole subtree beneath rolebase)
                      # {0} is substituted with the DN of the user
                      # {1} is substituted with the username
                      # {2} is substituted with an attribute value from user's directory entry, of the authenticated user. Use userroleattribute to specify the name of the attribute
                      rolesearch: '(member={0})'
                      # Specify the name of the attribute which value should be substituted with {2} above
                      userroleattribute: null
                      # Roles as an attribute of the user entry
                      userrolename: disabled
                      #userrolename: memberOf
                      # The attribute in a role entry containing the name of that role, Default is "name".
                      # Can also be "dn" to use the full DN as rolename.
                      rolename: cn
                      # Resolve nested roles transitive (roles which are members of other roles and so on ...)
                      resolve_nested_roles: true
                      userbase: 'ou=people,dc=example,dc=com'
                      # Filter to search for users (currently in the whole subtree beneath userbase)
                      # {0} is substituted with the username
                      usersearch: '(uid={0})'
                      # Skip users matching a user name, a wildcard or a regex pattern
                      #skip_users:
                      #  - 'cn=Michael Jackson,ou*people,o=TEST'
                      #  - '/\S*/'
                roles_from_another_ldap:
                  description: "Authorize via another Active Directory"
                  http_enabled: false
                  transport_enabled: false
                  authorization_backend:
                    type: ldap
  EOF
  ```

  Edit the contents of `config.yml` here to setup required authentication protocols.


* Install OpenSearch Helm chart:

  ```shell
  helm install opensearch ./opensearch --values values-opensearch.yaml
  ```


## Install OpenSearch Dashboard (Kibana free implementation)

### Download and install OpenSearch Dashboard Helm package

```shell
helm pull opensearch/opensearch-dashboards --untar
helm install opensearch-dashboards ./opensearch-dashboards
```

### Redirect pod port to the host

Demo (default) Opensearch configuration authenticates with insecure credentials:

* user: `admin`
* password: `admin`

Be careful: make sure it's safe to provide host access to OpenSearch Dashboard. Use `screen` application
or other virtual terminal on the host for more convenient managing background commands.

```shell
kubectl port-forward --address 0.0.0.0 opensearch-dashboards 8888:5601 &
```

## Install Logstash

### Download Logstash Helm chart

```
helm repo add bitnami https://charts.bitnami.com/bitnami
helm search repo logstash
helm pull bitnami/logstash --untar
```

Helm chart is extracted to `logstash` directory.

### Place Logstash pipeline configuration files to `logstash/files/config/*`

### Create `values-logstash.yaml` file

```
# Special Logstash image containing Opensearch output plugin
image:
  repository: opensearchproject/logstash-oss-with-opensearch-output-plugin
  tag: 7.16.2

#opensearchHost: opensearch-cluster-master-0.opensearch-cluster-master-headless.default.svc.cluster.local
opensearchHost: 192.168.4.105

# The configmap contains Logstash pipeline configuration files: inputs, filters, outputs
extraVolumes:
  - name: configs
    configMap:
      name: logstash-conf
extraVolumeMounts:
  - mountPath: /usr/share/logstash/pipeline
    name: configs
    readOnly: true

input: []
filter: []
output: []

service:
  type: LoadBalancer
  ports:
    http:
      port: 8080
      targetPort: http
      protocol: TCP
    default-port:
      port: 12201
      targetPort: 12201
      protocol: TCP
    http-port:
      port: 12202
      targetPort: 12202
      protocol: TCP
```

### Create a comfigmap template aggregating Logstash pipeline configuration files from `logstash/files/config/*`

```shell
vi logstash/templates/config.yaml
```
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "common.names.fullname" . }}-conf
  labels: {{- include "common.labels.standard" . | nindent 4 }}
data:
{{- $directory := "files/config/" }}
{{- range $path, $_ := $.Files.Glob (printf "%s*" $directory) }}
  {{ $path | trimPrefix $directory }}: |-
{{ tpl ($.Files.Get $path) $ | indent 4 }}
{{- end }}
```

The function `tpl` interpretes each pipeline configuration file as Helm template.
So for example it's possible to specify OpenSeacrh URL as `"https://{{ .Values.opensearchHost }}:9200"`.

### Install Logstash Helm package

```shell
helm install logstash bitnami/logstash ./logstash/ -f values-logstash.yaml
```
